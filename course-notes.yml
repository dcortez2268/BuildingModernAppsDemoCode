request offloading: allows you to respond to client without process being completely done yet 

s3 select API: feature of s3 that allows you to read a subset of data from s3 using SQL queries, helps save on computing costs and cheaper than downloading whole object

AWS systems manager parameter store: allows you to store key values pairs that are user defined, acts as a external resource file to store hard coded strings, secrets, and config data management

every AWS service has an API and you can use the command line or the SDK to interact with those APIs

// AWS is service
var AWS = require('aws-sdk')
const s3 = new AWS.S3({
    region: 'us-east-1'
});
client classes: static classes of AWS (service), S3 is client class

service objects: insantiated objects from client constructor, has methods to make calls from client classes, which call the AWS apis for you, 
s3 is service object

CREDENTIALS:
each api call needs key credentials, if you are using cloud9 it is configured with credentials
the SDK looks for credentials in the following order:
service client constructor,
environment variables,
shared credentials file,
amazon ECS credentials provider,
preferred: IAM role, configure role for ec2 instance, container task, or lambda function,

temporary credentials: Cloud9 automatically creates managed temporary credentials when its managed on an ec2 instance, has all permissions the current iam user has
can be disabled in preferences, then have to provide credentials by providing role for ec2 instance, add access key and secret key manually via CLI

serverless application: combination of Lambda functions, event resources, and other resources that work together to perform your distributed tasks

SAM, aws serverless application model: framework for building serverless applications, extension of Cloudformation
features:
shorthand syntax to express functions, APIs, databases, and more, then translates to Cloudformation template during deployment
integrates with AWS tools for building serverless applications

SAM Template specification: allows us to define the serverless application
SAM CLI: tools that allows us to build the applications defined by SAM template

benefits:
helps you organize related components and resources and operate on a single stack
allows you to use and enforce best practices such as code reviews
allows you to test and deploy the same environment



APIS:

API driven development: When creating a new service, the first artifact we build id the API
benefits:
back end and front end can be built in parallel once api is complete
allows us to provide a modular design 
aids in documentation because specifications need to be defined clearly

Websocket APIs: use low level protocols based on socket imports and requires use of IP addresses and port information, stateful, used for applications that need real time data or real time communication
HTTP APIs: enable you to create restful APIs with lower latency and lower costs than REST APIs, are used to proxy backend resources
REST APISs: fast, stateless, standard, horizontally scalable, and dependable, uses HTTPS, allows you to have full control over requests and responses between client and API gateway, 
    collection of resources and methods that are integrated with backend HTTP endpoints, Lambda function, or other AWS services

API Gateway: allows you to build and manage Rest, HTTP, and websocket APIS at any scale.
Features and benefits:
canary release deployments for safety, rolling out changes, logging and mointoring through Cloud trail and Cloudwatch, 
support for custom domain names, throttling of requests, and direct integration with other services such as WAF, X-RAY, and Lambda,
provides multiple endpoint types you can utilize
endpoint types: 
    edge optimized: best for geographically distributed clients, default type
    regional: intended for clients in same region,
    private: allow a client secure access to resources inside a VPC

resource: abstract concept that allows you to expose something to be consumed by a client 
integration type: backend that API sits in front of

Models and Mapping: API gateway REST Apis provide a way to validate incoming requests against models and data can be transformed in shape by using mappings

models: define the structure or shape of the payload of the request, use JSON schemas
    different methods could have different models
mappings: are applied to the integrations request and integration response of your API.  
    different methods could have different mappings
    written in VTL, or Velocity Template language, 
    stage variables: api gateway has default variables that are appended with $ to access

client => method request => integration request => integration type (backend) => integration response => method response => client :

method request: performs validation against models, can validate request parameters in URI, query string, and headers OR validate the payload adheres to model
integration request: performs mapping if needed
integration response: performs mapping if needed to encapuslate into HTTP wrapper
Method response: returns data from integration response to client,
 performs validation against models, can validate request parameters in URI, query string, and headers OR validate the payload adheres to model


 API deployment:
 deployment: must be created before a client can call your API and has an associated stage
 
 stage: is a reference to a deployment, or snapshot.  Helps you manage and optimize a particular deployment. every time you make a change to your api, you must deploy to a stage for it to go live. 
    provide: method for deployment, api versioning, performance management, and sdk generation
 
 stage variables: configuration attributes associated with a deployment stage.  they act like environment variables and can be used in your api setup and mapping templates.  Essentially they allow you to pass different variables for different methods to API backend, (integration)

 deployment resource: instantiated when you create deployment and is similar to an executable of an API.

 
API Authentication:
types authorization and authentication depend on API type
REST API authentication and authorization: 
IAM, Lambda, and Amazon Cognito user pools

IAM:
Under the stages tab, you can generate an sdk for different platforms.  typically used for server to server communication.  
attach an IAM role for your code to get authenticated.  

for native APIs you can add authentication and authorization via an access key and a secret key, the AWS SDK does it for you when making calls to AWS APIs, 

Cognito:
create pool then add in authorizers tab of API gateway.  web and mobile applications are authorized by cognito to make calls to api.
what is cognito?:
made up of two services, user pool and identity pool.  They can work together or individually.
user pool:  
identity provider, which is a user directory.  You can sign up users with e-mail or phone number validation, sign in, reset passwords, and perform MFA.  
Steps to authentication:
Client authenticates with Cognito user pool and gets JWT.  AWS created web pages for this workflow which you can customize.  
Client sends request to API gateway with JWT.
API validates the JWT with request to Cognito user pool.  Gateway then sends request to backend once the JWT has been validated

Can also be authenticated via federation, which means authenticating via a third party
Client authenticates with OIDC, SAML, or social providers and gets token
client sends request to cognito user pool with token
cognito user pool validates the token with OIDC, SAML, or social media platform. saves user info, then sends response to client with JWT token
client sends request to api gateway with jwt
gateway validates the JWT and then sends request to backend
identity pool (federated identities): 
allows us to assign IAM roles to users who authenticate through a separate identity provider, either OIDC, SAML, social media platform, or USER POOL 
contains a built in rule engine that allows you to select a specific iam role based on the data provided by the identity provider you are using
unauthenticated identity allows you to give default iam role for anyone that doesn't have credentials to authenticate
USE identity pool or user pool directly for authorization?:
use user pool when you only need to communicate with API gateway and you want your backend to have more information about user
use identity pool when you need to communicate with almost any other aws services directly
Triggers:
specify your own code to launch for pretty much any piece of workflow when it touches user pool. ex:you could run code to add user to database when they sign up
app client: the definition of the application that is going to use Cognito user pool as their database of users. 

Lambda authorizers: request parameters from client to gateway are sent to lambda, lambda returns a principle and a policy
principle: definition of this user obtained through authorization code in lambda
policy: like an IAM policy, that defines what the user can do
Gateway then authorizes the request and sends a 403 back to client if denied and sends request to backend if allows


access controls: 
many types, they are additions to authentication and authorization and you can combine many of them.  they include... 
resource policies applied at an gateway endpoint, endpoint policies for interface VPC endpoint, CORS, WAF, client side SSL certificate, and usage plans with API keys
client side: all are client side, besides client side SSL certificate

CORS: cross-origin resource sharing, browser security feature that restricts cross origin HTTP requests that are running in the browser,
headers specify which requests can be made from which url, 
cross-origin HTTP requests: whenever the url in browser is different than the url in script

AWS WAF: web application firewall that helps protect your API from common web exploits.  Has rules for managing top 10 security risks.  It can also be configured to filter any requests based on HTTP headers, IP address, or HTTP body.

usage plans: allows customers to access selected APIs at agreed upon request rates and quotas by implementing the creation of a key and the client inserts the api key as the value of the HTTP header X-API key 

Resource policies: allows you to create resource based policies to allow or deny access to your APIs and methods using iam condition elements. 

VPC endpoint policy: to access a private API from a VPC, you must implement a VPC interface endpoint.  The endpoints are powered by AWS private link, which enables you to privately access aws services by using private ip addresses.  You can add a policy for the VPC endpoint that allows or denies permissions for the API.  


HTTP APIs authentication and authorization:
JSON Web Tokens, (JWT), or JWT that are part of the openID Connect on OAuth2.0 protocols, or Amazon Cognito user pools because they use JWT as well
authorization scopes: allow you to further restrict certain routes to privilege users, they are like strings in the identity provider that provide access to route


//**************************************:
AWS LAMBDA:
//**************************************:

event: JSON formatted document that contains data for a function to process
concurrency: number of requests that your function is serving at any given time
trigger: resource or configuration that invokes a lambda function
    event source mapping: resource in lambda that reads items from a queue and invokes a function,
    aws services configured to invoke a function, and apps

Lambda function permissions:
execution permissions and resource-based policies

execution permissions: 
define what your lambda functions can do,
use IAM role based access, do not embed credentials into lambda execution environment
also needs basic execution permissions in order to do things like posting logs in cloudwatch, AWSLambdaBasicExecutionRole
resource based policies:
define who can invoke or manage your lambda function
you can allow or deny certain entities from interacting with your lambda function


Push and Pull Models for invoking Lambda:
Push: 
When a trigger sends an event to lambda
The resource based policy for Lambda must explicitly allow the invocation.
The call is made either asynchronously or synchronously, usually determined by the service, but sometime you can decide
    asynchronous call:
    performs asynchronous call when a response is not expected, like with batch jobs
        error handling: incoming events are placed in the queue before being sent to the actual fn, if the fn returns an error to lambda service then lambda will retry fn twice by default or keep event kept in the queue for a certain time period.  If either are reached, then you can have error sent to DLQ, or Lambda destinations
            DLQ: 
            holds results from error in SQS or sends to SNS topic
            Lambda destinations:
            something to notify after an asynchronous invocation did or did not work

    synchronous call:
    performs synchronous call when a response is expected, like with REST APIs
        error handling: you'll know if there was an error by response

Pull:
Lambda the service pulls events from event sources, and then invokes your lambda function. 
The event source mapping allows you to configure how lambda the service pulls the queue or stream, then delivers them to lambda function.
The execution permission must explicitly allow the appropriate allowed API calls


Lambda Execution Context Reuse:

execution context: 
code and objects declared outside the handler method, like imports etc
cold start: 
there is added latency when a function is executed because execution context needs to be "bootstrapped" 
warm start: 
after a function is executed, lambda maintains the execution context for 15 mins, "warming" the context for reuse so reduced latency when called again because context is reused
After 15 mins, lambda will terminate the execution context, and if called again will need to be bootstrapped again
provisioned concurrency:
feature that allows you to launch the amount of execution contexts you have specified and will keep them warm for you.  can be less expensive than running a Lambda on demand if you are using that capacity you reserved.


Compliance with AWS Lambda:

For encryption for data in flight: 
Lambda API endpoints only support secure connections over HTTPS
For encryption 
When modifying Lambda resources:
TLS is used for console, SDK, or API
For encryption for data at rest:
environment variables are stored within Lambda and encrypted
you can customize the encryption via key config and encryption helpers (client-side)
Elastic Network Interface, ENI: 
allows your lambda functions to access resources inside a VPC, while keeping the resources protected.  You tell Lambda which resources you need to connect to, and it does the rest.


Aliases and Versions:

Versions: 
help manage the deployment of functions, enabling the baility to publish a new version of a function for testing, without affecting stable prod version
with every version there are two ARNS, (references), one that points to the published fn and another that points to the copy
includes:
code and dependencies,  runtime setting and env variables, and ARN

alias: 
like a pointer to a specific lambda function version,
has ARN that is a reference to alias
very helpful when you have a trigger that points to lambda fn, and every time you make a change it would require you to change event mapping for that trigger, however with aliases you can keep the same ARN in the event mappings and modify the alias point to newest lambda version



//**************************************:
AWS STEP FUNCTIONS:
//**************************************:

What is it?:
serverless workflow tool that allows you to create a state machine that consists of one or many states
keeps track of the state of state machine and a history of executions of state machine
logs the state of each step so that you have the ability to debug
allows us to build loosely coupled system rather than having lambda functions be tightly coupled
state machine:
workflow, or collection of states which allow you to perform the work in sequence, passing the output of one worker as the input of another
state: 
individual elements of state machine, must have unique name
tasks:
workers, help your states accomplish their work

Step Function State Types:
task: do some work in your state machine
choice: make a choice between branches of execution
fail or succeed state: stop an execution with a failure or a success
pass: pass its input to its output or inject some fixed data, also for debugging
wait: provide a delay for a certain amount of time
parallel: begin parallel branches of execution
map: dynamically iterate steps

all besides fail state have full control over input and output.  
path: "$.variableName" is how you pull variables value from payload of JSON text in state machine


Step Function Service Integrations:
AWS Step Functions integrates with some AWS services so that you can call API actions, and coordinate executions directly from the States Language in step functions.  You can directly call and pass parameters to the APIs of those services.

Simply invoke with a task state.  Can integrate with many features and do some of the following:
invoke lambda function
run an aws batch job and then perform different actions based on the results
insert or get item from DynamoDB
run an ecs task and wait for it to complete 
publish to a topic in SNS
send a message in SQS
Manage a job for glue or sageMaker
execute EMR jobs
launch another step functions workflow

Patterns:
async:
send request to a service, progress to next state immediately after it gets HTTP response
callback tasks(synchronous):
call a service and have step functions wait for a job to complete 
call a service with a task token and wait until that token is returned with a payload


activity worker:
any external compute
activity task state type: 
task that gets performed by code running somewhere externally, like EC2
create an activity, which is like queue of tasks, and point to it in state
worker sends request to task, and task sends response with taskToken
worker needs to send periodic heartbeat until done, otherwise task will fail
when done, worker sends SendTaskSuccess or SendTaskFailure with response


Standard vs Express state functions:
standard and express are the two different workflows for state machine
express workflow:
ideal for high-volume, event-processing workloads such as IOT data ingestion, streaming data processing and transformation, and mobile application backends
over 100000 per second executions
nearly unlimited state transitions
does not persist state transitions to disk, only keeps them in memory
each state will run at least once, bc if there is issue it will have to restart entire machine
prices by number of executions you run, duration, and memory consumption
maximum duration is 5 minutes
execution history is sent to amazon cloudwatch logs
does not support job run or callback patterns, basically only supports asynchronous calls
standard workflow:
ideal for long-running, durable, and auditable workflows
over 2000 per second executions
over 4000 per second per account
each state transition is persisted to disk
each state will run only once, bc if there is issue it will restart execution at last state 
priced per state transition
maximum duration is 1 year
you can use console to see execution history or send to amazon cloudwatch logs
supports all service integrations and patterns

Best practices for step functions:
use timeouts to avoid stuck executions
use ARNS for s3 instead of passing large payloads
Handle Lambda Service exceptions
Avoid latency when polling for activity tasks
choose correctly between standard or express workflows


Event driven architectures:
uses events to trigger and communicate between decoupled services.  

event: 
change in state or an update.  Events can carry state or can be identifier, things that are sent as notification.  
producer:
publishes event to the event router
consumer:
receives the event from event router
event router:
pushes events to consumer

Services that act as routers:
step functions, SQS, SNS, EventBridge
SQS:
message queieng service that allows you to build loosely coupled systems and allows component to component communication using messages,
consumers poll the SQS queue regularly to see if there are messages in the queue
two types of queues:
standard:offers a maximum throughput that is unordered.  At least once processing.
FIFO:processed only once and in fifo order.
SNS:
it allows you to send emails and text messages from your applications and allows you to publish messages to a topic, which are endpoints
notify consumers that something was sent by a producer
Event Bridge Service:
service bus that helps to connect consumers and producers easily by implementing a predefined schema
maps schema to code so easy to work with inside your code
Schema Registry:a central repo, helps the producer tell the consumer what the schema look like




//**************************************:
Monitoring, Logging, observability:
//**************************************:

Monitoring: act of collecting data. we collect metrics, logs, watch network traffic, send alerts when something seems off.  

observability: 
allows you to measure how well the internal workings of a system are from the outputs of that system.  A system with high observability is desireable.  
provides a ssytem with a tight feedback look
how to implement: 
use distributed tracing software.  It alllows you to collect data on API calls made between components and measure the performance of each individual service and measure latency of the system as a whole.  


AWS X-Ray:
helps you analyze and debug distributed applications, such as those built using a microservices architecture,
high level of visibility allows you to identify and troubleshoot the root cause of performance issues and errors
provides a map of application's underlying components and a map of end to end requests as they travel through your application
implementation:
collects and records traces, which allows you to see service map with trace data.  This help you to identify any issues.
segments:
compute resources running your application logic, such as resource's name, details about requests and work done
subsegments:
provide more granular details such as timing information and downstreams calls by an app, you can add specific custom subsegments around certain lines of code
service graph:
JSON document that contains information about services and resources that make up your app. console uses this to generate service map
trace:
allows us to track the path of a request through app.  It collects all the segments generated by a single request.
integrations with other services:
if there is not integration with select service, x-ray provides inferred segments, but if there is integration then the service will provide segment
API gateway supports active tracing to analyze user requests
Lambda runs x-ray damon to records a segment about function invocation and execution
cloudtrail records api actions made by a user role or an aws service 
cloudwatch supports cloudwatch service lens and synthetics to monitor health of application
many more...

API Gateway with X-ray:
two types of tracing: active and passive
active tracing:
api gateway will automatically sample all api invocation requests based on sampling algorithm set in x-ray
passive tracing:
default setting, only traced if xray has been enabled on an upstream service.  tracing  will only capture upstream sampling

API Gateway with Lambda:
supports active and passive tracing as well 
X-Ray SDK:
library that provides functionality for creating and sending trace data to the X-Ray Daemon
x-ray daemon:
process that runs on a host machine that gathers raw trace segments and then periodically uploads that data to the x-ray API
implementation:
install dependencies
include statements is all that's needed in code


Cloudwatch Logs Integration with API Gateway, Step Functions, and Lambda:
Cloudwatch log event:
record of some activity recorded by service or resource being monitored.  Contains timestamp and data
log stream:
sequence of log events that share the same source.  generally one log stream per service instance
log group:
group of log streams that can share the same retention monitoring or access control settings.  ex for lambda you would have one log group per execution context
retention settings:
specify how long logs are kept. logs can cost money to store

all require role to send logs to cloudwatch log
how Lambda implements storing logs:
very simple, apply AWSLambdaBasicExecutionRole and logs will be enabled and could also use console.log to output data from code
how API gateway implements storing logs:
execution logging:allows you to see the traffic in and out of your API, api gateway manages  cloudwatch logs 
access logging:only available for rest api, more extensive, entails logging who has access to API and how caller accessed api, what it found from request after logging or transforming request
how step functions implements storing logs:
express workflows:configured by default
standard workflows:enable logging manually, has its own log history available for 90 days within console or you can send to cloudwatch logs





